{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f63422-b926-4761-bbd9-3422e91f33e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 DEFINE DIRECTORIES\n",
      "2.0 CLONE REPO AND INSTALL DIRECTORIES\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/accelerate_config/config.yaml\n",
      "kohya-trainer folder already exists\n",
      "Installation can take multiple minutes, enable \"Verbose\" to see progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 23.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:29:53.173631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 32\n",
      "Downloading model from https://civitai.com/api/download/models/29460...\n",
      "Failed to download the model from https://civitai.com/api/download/models/29460. Error: HTTP Error 403: Forbidden\n",
      "Model already exists.\n",
      "4.1 DATA CLEANING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 921.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been converted\n",
      "4.2.1. BLIP Captioning\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\n",
      "python /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/finetune/make_captions.py \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\" --batch_size=8 --beam_search --min_length=5 --max_length=75 --debug --caption_extension=\".caption\" --max_data_loader_n_workers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-07 17:30:07.412648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is:  /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer\n",
      "load images from /home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\n",
      "found 5 images.\n",
      "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "BLIP loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3/1.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3/2.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3/3.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3/4.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3/5.png a man with a beard and a white shirt\n",
      "done!\n",
      "4.2.3. Custom Caption/Tag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import configparser\n",
    "import random\n",
    "import concurrent.futures\n",
    "import time\n",
    "from library.studiolabs_utils import (\n",
    "    clone_or_update_repo,\n",
    "    install_dependencies,\n",
    "    create_dirs,\n",
    "    download_model,\n",
    "    preProcessingParams,\n",
    "    clean_directory,\n",
    "    process_image,\n",
    "    find_images,\n",
    "    convertImages,\n",
    "    preprocess_images,\n",
    "    run_captioning_process,\n",
    "    custom_caption_tag\n",
    ")\n",
    "\n",
    "\n",
    "print('1.0 DEFINE DIRECTORIES')\n",
    "dirs = create_dirs()\n",
    "print('2.0 CLONE REPO AND INSTALL DIRECTORIES')\n",
    "# Read the config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(dirs['trainer_config'])\n",
    "print(dirs['accelerate_config'])\n",
    "    \n",
    "clone_or_update_repo(\n",
    "    url=config.get('UserSettings', 'repo_url'),\n",
    "    save_directory=dirs['root_dir'],\n",
    "    branch = config.get('UserSettings', 'branch')\n",
    "    )\n",
    "\n",
    "\n",
    "install_dependencies(\n",
    "    dirs,\n",
    "    verbose=config.getboolean('UserSettings', 'verbose'), \n",
    "    install_xformers=config.getboolean('UserSettings', 'install_xformers')\n",
    "    )\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "command = \"pip cache purge\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "model_url = config.get('DownloadModels', 'model_url')\n",
    "vae_url = config.get('DownloadModels', 'vae_url')\n",
    "\n",
    "\n",
    "# Download the model file if it doesn't exist\n",
    "\n",
    "try:\n",
    "    download_model(model_url, dirs['pretrained_dir'])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download the model from {model_url}. Error: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    download_model(vae_url, dirs['vae_dir'])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download the VAE model from {vae_url}. Error: {str(e)}\")\n",
    "\n",
    "\n",
    "print('4.1 DATA CLEANING')\n",
    "train_image_folder = os.path.join(dirs['train_data_dir'],config.get('ImagePreprocessing', 'train_image_dir'))\n",
    "\n",
    "convert = config.get('ImagePreprocessing', 'convert')\n",
    "random_color = config.get('ImagePreprocessing', 'random_color')\n",
    "recursive = config.get('ImagePreprocessing', 'recursive')\n",
    "batch_size, supported_types, background_colors = preProcessingParams()\n",
    "\n",
    "clean_directory(train_image_folder, supported_types)\n",
    "images = find_images(train_image_folder)\n",
    "num_batches = len(images) // batch_size + 1\n",
    "convertImages(images,convert,batch_size,num_batches)\n",
    "\n",
    "print('4.2.1. BLIP Captioning')\n",
    "# Use BLIP for general images\n",
    "# Use Waifu for anime/manga images\n",
    "# Specified in the config file\n",
    "\n",
    "print(train_image_folder)\n",
    "run_captioning_process(config, train_image_folder, dirs['finetune_dir'])\n",
    "    \n",
    "print('4.2.3. Custom Caption/Tag')\n",
    "custom_caption_tag(config, train_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b9f6d-99d1-407b-9ba7-1d35c7d0abb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ## 5.5. Start Training\n",
    "\n",
    "#@markdown Check your config here if you want to edit something: \n",
    "#@markdown - `sample_prompt` : /content/dreambooth/config/sample_prompt.txt\n",
    "#@markdown - `config_file` : /content/dreambooth/config/config_file.toml\n",
    "#@markdown - `dataset_config` : /content/dreambooth/config/dataset_config.toml\n",
    "\n",
    "#@markdown Generated sample can be seen here: /content/dreambooth/output/sample\n",
    "\n",
    "#@markdown You can import config from another session if you want.\n",
    "\n",
    "import toml\n",
    "\n",
    "sample_prompt = os.path.join(dreambooth_training_dir,\"config/sample_prompt.txt\") #@param {type:'string'}\n",
    "config_file = os.path.join(dreambooth_training_dir,\"config/config_file.toml\") #@param {type:'string'}\n",
    "dataset_config = os.path.join(dreambooth_training_dir,\"config/dataset_config.toml\") #@param {type:'string'}\n",
    "\n",
    "with open(config_file, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "    \n",
    "config['model_arguments']['pretrained_model_name_or_path'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/pretrained_model/realisticVisionV20_v20NoVAE.safetensors\"\n",
    "config['model_arguments']['vae'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/pretrained_model/realisticVisionV20_v20NoVAE.safetensors\"\n",
    "config['huggingface_arguments']['huggingface_path_in_repo'] = \"mymodel\"\n",
    "config['huggingface_arguments']['huggingface_token'] = \"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\"\n",
    "config['huggingface_arguments']['huggingface_repo_id'] = \"xxthekingxx/myMiroslav7\"\n",
    "config['training_arguments']['output_name'] = \"Miroslav7\"\n",
    "config['training_arguments']['log_prefix'] = \"Miroslav7\"\n",
    "config['training_arguments']['max_train_steps'] = 2500\n",
    "config['optimizer_arguments']['learning_rate'] = 1e-6\n",
    "    \n",
    "with open(config_file, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(dataset_config, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "\n",
    "print(config['datasets'][0]['subsets'][0]['image_dir'])\n",
    "config['datasets'][0]['subsets'][0]['image_dir'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\"\n",
    "\n",
    "with open(dataset_config, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "\n",
    "\n",
    "accelerate_conf = {\n",
    "    \"config_file\" : accelerate_config,\n",
    "    \"num_cpu_threads_per_process\" : 1,\n",
    "}\n",
    "\n",
    "train_conf = {\n",
    "    \"sample_prompts\" : sample_prompt,\n",
    "    \"dataset_config\" : dataset_config,\n",
    "    \"config_file\" : config_file\n",
    "}\n",
    "\n",
    "def train(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    return args\n",
    "\n",
    "accelerate_args = train(accelerate_conf)\n",
    "train_args = train(train_conf)\n",
    "final_args = f\"accelerate launch {accelerate_args} train_db.py {train_args}\"\n",
    "print(final_args)\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95deb00f-ea37-4c39-84bf-9ecd4034a560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fonttools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b044b58-21c1-4f6b-9c5a-0edc677a1071",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def get_font():\n",
    "    # Get the list of font names\n",
    "    font_names = [f.name for f in fm.fontManager.ttflist]\n",
    "    #print([(i,k) for i,k in enumerate(font_names)])\n",
    "    font_path = fm.findfont(font_names[17])\n",
    "    fontsize = 40\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "    return font, fontsize\n",
    "\n",
    "def fetch_image_locations(directory):\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = [f for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    return image_locations\n",
    "\n",
    "def main():\n",
    "    top_space = 40\n",
    "    left_space = 400\n",
    "    text_offset = 4\n",
    "    padding = 20  # Padding between images\n",
    "    text_width_limit = 750 \n",
    "    # Directory path and grid settings\n",
    "    directory = '/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav7/'\n",
    "    grid_save_dir = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav7_grid.png\"\n",
    "\n",
    "\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = fetch_image_locations(directory)\n",
    "    prompts = sorted(list(set([img_name.split('_')[1] for img_name in image_locations])))\n",
    "    epochs = sorted(list(set([img_name.split('_')[2][1:] for img_name in image_locations])))\n",
    "    font, fontsize = get_font()\n",
    "    \n",
    "    max_image_width = 0\n",
    "    max_image_height = 0\n",
    "\n",
    "    # Find the maximum width and height among all images\n",
    "    for image_file in image_locations:\n",
    "        image_path = os.path.join(directory, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        max_image_width = max(max_image_width, width)\n",
    "        max_image_height = max(max_image_height, height)\n",
    "\n",
    "    canvas_width = ((max_image_width + padding) * len(epochs)) + padding + left_space\n",
    "    canvas_height = ((max_image_height + padding) * len(prompts)) + padding + top_space\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Iterate over the image files and place them in the grid\n",
    "    for i,epoch in enumerate(epochs):\n",
    "        epoch_x = ((max_image_width + padding) * i) + padding + left_space \n",
    "        epoch_y = text_offset \n",
    "        draw.text((epoch_x, epoch_y), f'Epoch: {epoch}', fill='black', font=font)\n",
    "        for k,prompt in enumerate(prompts):\n",
    "        \n",
    "            text=prompt\n",
    "            #Shorten the text if it exceeds 50 characters\n",
    "            if len(text) > 200:\n",
    "                text = text[:200] + '...'\n",
    "\n",
    "            # Wrap the text to fit within the limit\n",
    "            wrapped_text = textwrap.wrap(text, width=int(text_width_limit / fontsize))\n",
    "\n",
    "            # Calculate the total height required for the wrapped text\n",
    "            total_text_height = len(wrapped_text) * fontsize\n",
    "\n",
    "            # Calculate the starting position to center the text vertically\n",
    "            y_start = top_space + padding + k * (padding + max_image_height)\n",
    "            x = text_offset * 4\n",
    "\n",
    "            # Draw the wrapped text\n",
    "            for line in wrapped_text:\n",
    "                text_bbox = draw.textbbox((0, y_start), line, font=font)\n",
    "                text_width = text_bbox[2] - text_bbox[0]\n",
    "                text_height = text_bbox[3] - text_bbox[1]\n",
    "                draw.text((x, y_start), line, font=font, fill='black')\n",
    "                y_start += fontsize\n",
    "        \n",
    "            for img_name in image_locations:\n",
    "                if epoch == img_name.split('_')[2][1:] and prompt == img_name.split('_')[1]:\n",
    "                    image_path = os.path.join(directory, img_name)\n",
    "                    img = Image.open(image_path)\n",
    "\n",
    "                    # Calculate the position of the image in the grid\n",
    "                    x = (max_image_width + padding) * i + padding + left_space\n",
    "                    y = top_space + (max_image_height + padding) * k + padding\n",
    "\n",
    "                    # Paste the image onto the canvas\n",
    "                    canvas.paste(img, (x, y))\n",
    "\n",
    "    # Save the final image grid\n",
    "    canvas.save(grid_save_dir)\n",
    "    print('Saved')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937f88d-819a-4e35-bc36-353198d18048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 6.2. Inference\n",
    "v2 = False  # @param {type:\"boolean\"}\n",
    "v_parameterization = False  # @param {type:\"boolean\"}\n",
    "prompt = \"RAW photo, mirox in a fancy suit, fashion magazine photoshoot, full body shot, high detailed skin, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"  # @param {type: \"string\"}\n",
    "negative = \"(weird eyes, disfigured eyes, looking different direction:1.3), cgi, 3d, render, mutated hands, mutated fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, bad quality, worst quality\"  # @param {type: \"string\"}\n",
    "model = os.path.join(dreambooth_output_dir,'Miroslav7.ckpt')  # @param {type: \"string\"}\n",
    "vae = os.path.join(vae_dir,'vae-ft-mse-840000-ema-pruned.ckpt')  # @param {type: \"string\"}\n",
    "outdir = inference_dir  # @param {type: \"string\"}\n",
    "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
    "sampler = \"euler_a\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "steps = 35  # @param {type: \"slider\", min: 1, max: 100}\n",
    "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
    "width = 512  # @param {type: \"integer\"}\n",
    "height = 768  # @param {type: \"integer\"}\n",
    "images_per_prompt = 12  # @param {type: \"integer\"}\n",
    "batch_size = 1  # @param {type: \"integer\"}\n",
    "clip_skip = 1  # @param {type: \"slider\", min: 1, max: 40}\n",
    "seed = -1  # @param {type: \"integer\"}\n",
    "\n",
    "final_prompt = f\"{prompt} --n {negative}\"\n",
    "\n",
    "config = {\n",
    "    \"v2\": v2,\n",
    "    \"v_parameterization\": v_parameterization,\n",
    "    \"ckpt\": model,\n",
    "    \"outdir\": outdir,\n",
    "    \"xformers\": True,\n",
    "    \"vae\": vae if vae else None,\n",
    "    \"fp16\": True,\n",
    "    \"W\": width,\n",
    "    \"H\": height,\n",
    "    \"seed\": seed if seed > 0 else None,\n",
    "    \"scale\": scale,\n",
    "    \"sampler\": sampler,\n",
    "    \"steps\": steps,\n",
    "    \"max_embeddings_multiples\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"images_per_prompt\": images_per_prompt,\n",
    "    \"clip_skip\": clip_skip if not v2 else None,\n",
    "    \"prompt\": final_prompt,\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    if isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    if isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    if isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python gen_img_diffusers.py {args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec053eed-bc75-4fce-a6b4-1362f5e86a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#@title ## 7.2. Model Pruner\n",
    "\n",
    "os.chdir(tools_dir)\n",
    "\n",
    "if not os.path.exists('prune.py'):\n",
    "    !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
    "\n",
    "#@markdown Convert to Float16\n",
    "fp16 = True #@param {'type':'boolean'}\n",
    "#@markdown Use EMA for weights\n",
    "ema = False #@param {'type':'boolean'}\n",
    "#@markdown Strip CLIP weights\n",
    "no_clip = False #@param {'type':'boolean'}\n",
    "#@markdown Strip VAE weights\n",
    "no_vae = False #@param {'type':'boolean'}\n",
    "#@markdown Strip depth model weights\n",
    "no_depth = False #@param {'type':'boolean'}\n",
    "#@markdown Strip UNet weights\n",
    "no_unet = False #@param {'type':'boolean'}\n",
    "\n",
    "model_path = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/Hen1.ckpt\" #@param {'type' : 'string'}\n",
    "\n",
    "config = {\n",
    "    \"fp16\": fp16,\n",
    "    \"ema\": ema,\n",
    "    \"no_clip\": no_clip,\n",
    "    \"no_vae\": no_vae,\n",
    "    \"no_depth\": no_depth,\n",
    "    \"no_unet\": no_unet,\n",
    "}\n",
    "\n",
    "suffixes = {\n",
    "    \"fp16\": \"-fp16\",\n",
    "    \"ema\": \"-ema\",\n",
    "    \"no_clip\": \"-no-clip\",\n",
    "    \"no_vae\": \"-no-vae\",\n",
    "    \"no_depth\": \"-no-depth\",\n",
    "    \"no_unet\": \"-no-unet\",\n",
    "}\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "\n",
    "dir_name = os.path.dirname(model_path)\n",
    "base_name = os.path.basename(model_path)\n",
    "output_name = base_name.split('.')[0]\n",
    "\n",
    "for option, suffix in suffixes.items():\n",
    "    if config[option]:\n",
    "        print(f\"Applying option {option}\")\n",
    "        output_name += suffix\n",
    "        \n",
    "output_name += '-pruned'\n",
    "output_path = os.path.join(dir_name, output_name + ('.ckpt' if model_path.endswith(\".ckpt\") else \".safetensors\"))\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python3 prune.py {model_path} {output_path} {args}\"\n",
    "!{final_args}\n",
    "\n",
    "print(f\"Saving pruned model to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39252798-c427-4770-a350-465cb6b62bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 7.1. Upload Config widgimport ipywidgets as widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create the widget elements\n",
    "write_token_input = widgets.Text(description=\"Write Token:\",style={\"description_width\": \"initial\"})\n",
    "orgs_name_input = widgets.Text(description=\"Orgs Name:\",style={\"description_width\": \"initial\"})\n",
    "model_name_input = widgets.Text(description=\"Model Name:\",style={\"description_width\": \"initial\"})\n",
    "dataset_name_input = widgets.Text(description=\"Dataset Name:\",style={\"description_width\": \"initial\"})\n",
    "make_private_checkbox = widgets.Checkbox(value=True, description=\"Make Private\")\n",
    "\n",
    "# Create the widget container\n",
    "widget_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Login to Huggingface Hub</h3>\"),\n",
    "    write_token_input,\n",
    "    orgs_name_input,\n",
    "    model_name_input,\n",
    "    dataset_name_input,\n",
    "    make_private_checkbox\n",
    "])\n",
    "\n",
    "display(widget_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a51b3-8edb-441e-9e51-fcfee60905a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 7.1. Create repo\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
    "\n",
    "\n",
    "# @markdown Login to Huggingface Hub\n",
    "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
    "write_token = write_token_input.value\n",
    "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
    "orgs_name = orgs_name_input.value\n",
    "# @markdown If your model/dataset repo does not exist, it will automatically create it.\n",
    "model_name = model_name_input.value\n",
    "dataset_name = dataset_name_input.value\n",
    "make_private = make_private_checkbox.value  # @param{type:\"boolean\"}\n",
    "\n",
    "def authenticate(write_token):\n",
    "    login(write_token, add_to_git_credential=True)\n",
    "    api = HfApi()\n",
    "    return api.whoami(write_token), api\n",
    "\n",
    "\n",
    "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
    "    global model_repo\n",
    "    global datasets_repo\n",
    "    \n",
    "    if orgs_name == \"\":\n",
    "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
    "    else:\n",
    "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
    "\n",
    "    try:\n",
    "        validate_repo_id(repo_id)\n",
    "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
    "    except HfHubHTTPError as e:\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
    "    \n",
    "    if repo_type == \"model\":\n",
    "        model_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
    "    else:\n",
    "        datasets_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
    "\n",
    "user, api = authenticate(write_token)\n",
    "\n",
    "# @markdown This will be uploaded to model repo\n",
    "#model_path = os.path.join(dreambooth_output_dir,\"Hen1.ckpt\")  # @param {type :\"string\"}\n",
    "path_in_repo = \"\"  # @param {type :\"string\"}\n",
    "# @markdown Now you can save your config file for future use\n",
    "# @markdown Other Information\n",
    "commit_message = \"uploading model\"  # @param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "    commit_message = \"feat: upload \" + project_name.value + \" checkpoint\"\n",
    "\n",
    "if model_name:\n",
    "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
    "if dataset_name:\n",
    "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)\n",
    "\n",
    "print(\"uploading to: \",user[\"name\"] + \"/\" + model_name.strip())\n",
    "print(\"uploading config\")\n",
    "api.upload_folder(\n",
    "    folder_path=dreambooth_config_dir,\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"config\",\n",
    ")\n",
    "print(\"uploading sample images\")\n",
    "api.upload_folder(\n",
    "    folder_path=dreambooth_output_dir + \"/\" + \"sample\" + \"/\" + \"Miroslav7\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"samples\",\n",
    ")\n",
    "print(\"uploading sample image grid\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=dreambooth_output_dir + \"/\" + \"sample\" + \"/\" + \"Miroslav7_grid.png\",\n",
    "    path_in_repo=\"Miroslav7_grid.png\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    ")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38929646-2133-4704-a8c9-dbc6b1b45f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ### 8.2.1. Upload Model\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# @markdown This will be uploaded to model repo\n",
    "#model_path = os.path.join(dreambooth_output_dir,\"Hen1.ckpt\")  # @param {type :\"string\"}\n",
    "path_in_repo = \"\"  # @param {type :\"string\"}\n",
    "# @markdown Now you can save your config file for future use\n",
    "config_path = dreambooth_config_dir  # @param {type :\"string\"}\n",
    "# @markdown Other Information\n",
    "commit_message = \"uploading model\"  # @param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "    commit_message = \"feat: upload \" + project_name.value + \" checkpoint\"\n",
    "\n",
    "#if os.path.exists(model_path):\n",
    "#    vae_exists = os.path.exists(os.path.join(model_path, \"vae\"))\n",
    "#    unet_exists = os.path.exists(os.path.join(model_path, \"unet\"))\n",
    "#    text_encoder_exists = os.path.exists(os.path.join(model_path, \"text_encoder\"))\n",
    "\n",
    "\n",
    "def upload_model(model_paths, is_folder: bool, is_config: bool):\n",
    "    path_obj = Path(model_paths)\n",
    "    trained_model = path_obj.parts[-1]\n",
    "\n",
    "    if path_in_repo:\n",
    "        trained_model = path_in_repo\n",
    "\n",
    "    if is_config:\n",
    "        if path_in_repo:\n",
    "            trained_model = f\"{path_in_repo}_config\"\n",
    "        else:\n",
    "            trained_model = f\"{project_name.value}_config\"\n",
    "\n",
    "    if is_folder == True:\n",
    "        print(f\"Uploading {trained_model} to https://huggingface.co/\" + model_repo)\n",
    "        print(f\"Please wait...\")\n",
    "\n",
    "        if vae_exists and unet_exists and text_encoder_exists:\n",
    "            api.upload_folder(\n",
    "                folder_path=model_paths,\n",
    "                repo_id=model_repo,\n",
    "                commit_message=commit_message,\n",
    "                ignore_patterns=\".ipynb_checkpoints\",\n",
    "            )\n",
    "        else:\n",
    "            api.upload_folder(\n",
    "                folder_path=model_paths,\n",
    "                path_in_repo=trained_model,\n",
    "                repo_id=model_repo,\n",
    "                commit_message=commit_message,\n",
    "                ignore_patterns=\".ipynb_checkpoints\",\n",
    "            )\n",
    "        print(\n",
    "            f\"Upload success, located at https://huggingface.co/\"\n",
    "            + model_repo\n",
    "            + \"/tree/main\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Uploading {trained_model} to https://huggingface.co/\" + model_repo)\n",
    "        print(f\"Please wait...\")\n",
    "\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=model_paths,\n",
    "            path_in_repo=trained_model,\n",
    "            repo_id=model_repo,\n",
    "            commit_message=commit_message,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Upload success, located at https://huggingface.co/\"\n",
    "            + model_repo\n",
    "            + \"/blob/main/\"\n",
    "            + trained_model\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "def upload():\n",
    "    #if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
    "    #    upload_model(model_path, False, False)\n",
    "    #else:\n",
    "    #    upload_model(model_path, True, False)\n",
    "\n",
    "    if config_path:\n",
    "        upload_model(config_path, True, True)\n",
    "\n",
    "\n",
    "upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082353d0-cb86-4d05-8d23-0e3bb8d33ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
