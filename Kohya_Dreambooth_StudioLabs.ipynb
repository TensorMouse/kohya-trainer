{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f63422-b926-4761-bbd9-3422e91f33e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.0 DEFINE DIRECTORIES\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import configparser\n",
    "import random\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from library.studiolabs_utils import (\n",
    "    clone_or_update_repo,\n",
    "    install_dependencies,\n",
    "    create_dirs,\n",
    "    download_model,\n",
    "    preProcessingParams(),\n",
    "    clean_directory,\n",
    "    process_image,\n",
    "    find_images,\n",
    "    convertImages\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "dirs = create_dirs()\n",
    "# 2.0 CLONE REPO AND INSTALL DIRECTORIES\n",
    "# Read the config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(dirs['trainer_config'])\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "repo_url = config.get('UserSettings', 'repo_url')\n",
    "branch = config.get('UserSettings', 'branch')\n",
    "install_xformers = config.getboolean('UserSettings', 'install_xformers')\n",
    "verbose = config.getboolean('UserSettings', 'verbose')\n",
    "\n",
    "    \n",
    "clone_or_update_repo(\n",
    "    url=repo_url,\n",
    "    save_directory=repo_dir\n",
    "    )\n",
    "\n",
    "\n",
    "install_dependencies(\n",
    "    install_dir=repo_dir,\n",
    "    verbose=verbose, \n",
    "    install_xformers=install_xformers\n",
    "    )\n",
    "\n",
    "\n",
    "command = \"pip cache purge\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "model_url = config.get('DownloadModels', 'model_url')\n",
    "vae_url = config.get('DownloadModels', 'vae_url')\n",
    "\n",
    "\n",
    "# Download the model file if it doesn't exist\n",
    "download_model(model_url, dirs['pretrained_dir'])\n",
    "download_model(vae_url, dirs['vae_dir'])\n",
    "\n",
    "#4.1 DATA CLEANING\n",
    "train_image_folder = os.path.join(dirs['train_data_dir'],config.get('ImagePreprocessing', 'train_image_dir'))\n",
    "convert = config.get('ImagePreprocessing', 'convert')\n",
    "random_color = config.get('ImagePreprocessing', 'random_color')\n",
    "recursive = config.get('ImagePreprocessing', 'recursive')\n",
    "batch_size, supported_types, background_colors = preProcessingParams()\n",
    "\n",
    "clean_directory(train_image_folder)\n",
    "images = find_images(train_image_folder)\n",
    "num_batches = len(images) // batch_size + 1\n",
    "convertImages(images,batch_size,num_batches)\n",
    "\n",
    "#@title ### 4.2.1. BLIP Captioning\n",
    "# Use BLIP for general images\n",
    "# Use Waifu for anime/manga images\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "\n",
    "if config.get('Captioning', 'captioning') == \"BLIP\":\n",
    "    ArgsConfig = {\n",
    "        \"_train_data_dir\" : train_data_dir,\n",
    "        \"batch_size\" : config.getint('Captioning', 'batch_size'),\n",
    "        \"beam_search\" : config.getboolean('Captioning', 'beam_search'),\n",
    "        \"min_length\" : config.getint('Captioning', 'min_length'),\n",
    "        \"max_length\" : config.getint('Captioning', 'max_length'),\n",
    "        \"debug\" : config.getboolean('Captioning', 'verbose_logging'),\n",
    "        \"caption_extension\" : config.get('Captioning', 'caption_extension'),\n",
    "        \"max_data_loader_n_workers\" : config.getint('Captioning', 'max_data_loader_n_workers'),\n",
    "        \"recursive\" : config.getboolean('Captioning', 'recursive')\n",
    "    }\n",
    "\n",
    "elif config.get('Captioning', 'captioning') == \"Waifu\":\n",
    "    ArgsConfig = {\n",
    "        \"_train_data_dir\": train_data_dir,\n",
    "        \"batch_size\": config.getint('Captioning', 'batch_size'),\n",
    "        \"repo_id\": config.get('Captioning', 'model'),\n",
    "        \"recursive\": config.getboolean('Captioning', 'recursive'),\n",
    "        \"remove_underscore\": True,\n",
    "        \"general_threshold\": config.getfloat('Captioning', 'general_threshold'),\n",
    "        \"character_threshold\": config.getfloat('Captioning', 'character_threshold'),\n",
    "        \"caption_extension\": config.get('Captioning', 'caption_extension'),\n",
    "        \"max_data_loader_n_workers\": config.getint('Captioning', 'max_data_loader_n_workers'),\n",
    "        \"debug\": config.getboolean('Captioning', 'verbose_logging'),\n",
    "        \"undesired_tags\": config.get('Captioning', 'undesired_tags')\n",
    "    }\n",
    "\n",
    "else:\n",
    "    print(\"No captioning option selected. Skipping captioning process.\")\n",
    "\n",
    "if config.get('Captioning', 'captioning') == \"BLIP\" or config.get('Captioning', 'captioning') == \"Waifu\":\n",
    "    args = \"\"\n",
    "    for k, v in ArgsConfig.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "    BLIP_captions = os.path.join(finetune_dir, 'make_captions.py')\n",
    "    WAIFU_captions = os.path.join(finetune_dir, 'tag_images_by_wd14_tagger.py')\n",
    "    final_args = f\"python {BLIP_captions} {args}\" if config.get('Captioning', 'captioning') == \"BLIP\" else f\"python {WAIFU_captions} {args}\"\n",
    "\n",
    "    !{final_args}\n",
    "    \n",
    "    \n",
    "# @title ### 4.2.3. Custom Caption/Tag\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "# Read the trainer_config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(repo_dir, 'trainer_config.ini'))\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "extension = config.get('CustomCaptionTag', 'extension')\n",
    "custom_tag = config.get('CustomCaptionTag', 'custom_tag')\n",
    "keywords = config.get('CustomCaptionTag', 'keywords').split(',')\n",
    "sub_folder = config.get('CustomCaptionTag', 'sub_folder')\n",
    "append = config.getboolean('CustomCaptionTag', 'append')\n",
    "prefix_tag = config.getboolean('CustomCaptionTag', 'prefix_tag')\n",
    "remove_tag = config.getboolean('CustomCaptionTag', 'remove_tag')\n",
    "recursive = config.getboolean('CustomCaptionTag', 'recursive')\n",
    "\n",
    "os.chdir(root_dir)\n",
    "\n",
    "if sub_folder == \"None\":\n",
    "    image_dir = train_data_dir\n",
    "elif sub_folder == \"--all\":\n",
    "    image_dir = train_data_dir\n",
    "    recursive = True\n",
    "else:\n",
    "    image_dir = os.path.join(train_data_dir, sub_folder)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "def write_file(filename, contents):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "def process_tags(filename, custom_tag, append, prefix_tag, remove_tag, keyword):\n",
    "    contents = read_file(filename)\n",
    "    tags = [tag.strip() for tag in contents.split(',')]\n",
    "    custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
    "    for custom_tag in custom_tags:\n",
    "        custom_tag = custom_tag.replace(\"_\", \" \")\n",
    "        if remove_tag:\n",
    "            while custom_tag in tags:\n",
    "                tags.remove(custom_tag)\n",
    "        else:\n",
    "            for i in range(len(tags)):\n",
    "                for keyword in keywords:\n",
    "                    if keyword in tags[i]:\n",
    "                        tags[i] = tags[i].replace(keyword, custom_tag)\n",
    "            if append:\n",
    "                tags.append(custom_tag)\n",
    "            if prefix_tag:\n",
    "                tags.insert(0, custom_tag)\n",
    "\n",
    "    contents = ', '.join(tags)\n",
    "    write_file(filename, contents)\n",
    "\n",
    "def process_directory(image_dir, tag, append, prefix_tag, remove_tag, recursive, keyword):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        file_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        if os.path.isdir(file_path) and recursive:\n",
    "            process_directory(file_path, tag, append, prefix_tag, remove_tag, recursive, keyword)\n",
    "        elif filename.endswith(extension):\n",
    "            process_tags(file_path, tag, append, prefix_tag, remove_tag, keyword)\n",
    "\n",
    "tag = custom_tag\n",
    "\n",
    "if not any(\n",
    "    [filename.endswith(extension) for filename in os.listdir(image_dir)]\n",
    "):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
    "            open(\n",
    "                os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
    "                \"w\",\n",
    "            ).close()\n",
    "\n",
    "if custom_tag:\n",
    "    process_directory(image_dir, tag, append, prefix_tag, remove_tag, recursive, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b9f6d-99d1-407b-9ba7-1d35c7d0abb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ## 5.5. Start Training\n",
    "\n",
    "#@markdown Check your config here if you want to edit something: \n",
    "#@markdown - `sample_prompt` : /content/dreambooth/config/sample_prompt.txt\n",
    "#@markdown - `config_file` : /content/dreambooth/config/config_file.toml\n",
    "#@markdown - `dataset_config` : /content/dreambooth/config/dataset_config.toml\n",
    "\n",
    "#@markdown Generated sample can be seen here: /content/dreambooth/output/sample\n",
    "\n",
    "#@markdown You can import config from another session if you want.\n",
    "\n",
    "import toml\n",
    "\n",
    "sample_prompt = os.path.join(dreambooth_training_dir,\"config/sample_prompt.txt\") #@param {type:'string'}\n",
    "config_file = os.path.join(dreambooth_training_dir,\"config/config_file.toml\") #@param {type:'string'}\n",
    "dataset_config = os.path.join(dreambooth_training_dir,\"config/dataset_config.toml\") #@param {type:'string'}\n",
    "\n",
    "with open(config_file, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "    \n",
    "config['model_arguments']['pretrained_model_name_or_path'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/pretrained_model/realisticVisionV20_v20NoVAE.safetensors\"\n",
    "config['model_arguments']['vae'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/pretrained_model/realisticVisionV20_v20NoVAE.safetensors\"\n",
    "config['huggingface_arguments']['huggingface_path_in_repo'] = \"mymodel\"\n",
    "config['huggingface_arguments']['huggingface_token'] = \"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\"\n",
    "config['huggingface_arguments']['huggingface_repo_id'] = \"xxthekingxx/myMiroslav7\"\n",
    "config['training_arguments']['output_name'] = \"Miroslav7\"\n",
    "config['training_arguments']['log_prefix'] = \"Miroslav7\"\n",
    "config['training_arguments']['max_train_steps'] = 2500\n",
    "config['optimizer_arguments']['learning_rate'] = 1e-6\n",
    "    \n",
    "with open(config_file, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(dataset_config, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "\n",
    "print(config['datasets'][0]['subsets'][0]['image_dir'])\n",
    "config['datasets'][0]['subsets'][0]['image_dir'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\"\n",
    "\n",
    "with open(dataset_config, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "\n",
    "\n",
    "accelerate_conf = {\n",
    "    \"config_file\" : accelerate_config,\n",
    "    \"num_cpu_threads_per_process\" : 1,\n",
    "}\n",
    "\n",
    "train_conf = {\n",
    "    \"sample_prompts\" : sample_prompt,\n",
    "    \"dataset_config\" : dataset_config,\n",
    "    \"config_file\" : config_file\n",
    "}\n",
    "\n",
    "def train(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    return args\n",
    "\n",
    "accelerate_args = train(accelerate_conf)\n",
    "train_args = train(train_conf)\n",
    "final_args = f\"accelerate launch {accelerate_args} train_db.py {train_args}\"\n",
    "print(final_args)\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95deb00f-ea37-4c39-84bf-9ecd4034a560",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fonttools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.39.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install fonttools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b044b58-21c1-4f6b-9c5a-0edc677a1071",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def get_font():\n",
    "    # Get the list of font names\n",
    "    font_names = [f.name for f in fm.fontManager.ttflist]\n",
    "    #print([(i,k) for i,k in enumerate(font_names)])\n",
    "    font_path = fm.findfont(font_names[17])\n",
    "    fontsize = 40\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "    return font, fontsize\n",
    "\n",
    "def fetch_image_locations(directory):\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = [f for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    return image_locations\n",
    "\n",
    "def main():\n",
    "    top_space = 40\n",
    "    left_space = 400\n",
    "    text_offset = 4\n",
    "    padding = 20  # Padding between images\n",
    "    text_width_limit = 750 \n",
    "    # Directory path and grid settings\n",
    "    directory = '/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav7/'\n",
    "    grid_save_dir = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav7_grid.png\"\n",
    "\n",
    "\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = fetch_image_locations(directory)\n",
    "    prompts = sorted(list(set([img_name.split('_')[1] for img_name in image_locations])))\n",
    "    epochs = sorted(list(set([img_name.split('_')[2][1:] for img_name in image_locations])))\n",
    "    font, fontsize = get_font()\n",
    "    \n",
    "    max_image_width = 0\n",
    "    max_image_height = 0\n",
    "\n",
    "    # Find the maximum width and height among all images\n",
    "    for image_file in image_locations:\n",
    "        image_path = os.path.join(directory, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        max_image_width = max(max_image_width, width)\n",
    "        max_image_height = max(max_image_height, height)\n",
    "\n",
    "    canvas_width = ((max_image_width + padding) * len(epochs)) + padding + left_space\n",
    "    canvas_height = ((max_image_height + padding) * len(prompts)) + padding + top_space\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Iterate over the image files and place them in the grid\n",
    "    for i,epoch in enumerate(epochs):\n",
    "        epoch_x = ((max_image_width + padding) * i) + padding + left_space \n",
    "        epoch_y = text_offset \n",
    "        draw.text((epoch_x, epoch_y), f'Epoch: {epoch}', fill='black', font=font)\n",
    "        for k,prompt in enumerate(prompts):\n",
    "        \n",
    "            text=prompt\n",
    "            #Shorten the text if it exceeds 50 characters\n",
    "            if len(text) > 200:\n",
    "                text = text[:200] + '...'\n",
    "\n",
    "            # Wrap the text to fit within the limit\n",
    "            wrapped_text = textwrap.wrap(text, width=int(text_width_limit / fontsize))\n",
    "\n",
    "            # Calculate the total height required for the wrapped text\n",
    "            total_text_height = len(wrapped_text) * fontsize\n",
    "\n",
    "            # Calculate the starting position to center the text vertically\n",
    "            y_start = top_space + padding + k * (padding + max_image_height)\n",
    "            x = text_offset * 4\n",
    "\n",
    "            # Draw the wrapped text\n",
    "            for line in wrapped_text:\n",
    "                text_bbox = draw.textbbox((0, y_start), line, font=font)\n",
    "                text_width = text_bbox[2] - text_bbox[0]\n",
    "                text_height = text_bbox[3] - text_bbox[1]\n",
    "                draw.text((x, y_start), line, font=font, fill='black')\n",
    "                y_start += fontsize\n",
    "        \n",
    "            for img_name in image_locations:\n",
    "                if epoch == img_name.split('_')[2][1:] and prompt == img_name.split('_')[1]:\n",
    "                    image_path = os.path.join(directory, img_name)\n",
    "                    img = Image.open(image_path)\n",
    "\n",
    "                    # Calculate the position of the image in the grid\n",
    "                    x = (max_image_width + padding) * i + padding + left_space\n",
    "                    y = top_space + (max_image_height + padding) * k + padding\n",
    "\n",
    "                    # Paste the image onto the canvas\n",
    "                    canvas.paste(img, (x, y))\n",
    "\n",
    "    # Save the final image grid\n",
    "    canvas.save(grid_save_dir)\n",
    "    print('Saved')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f937f88d-819a-4e35-bc36-353198d18048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-05 10:54:09.653546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "load StableDiffusion checkpoint\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "loading text encoder: <All keys matched successfully>\n",
      "load VAE: /home/studio-lab-user/sagemaker-studiolab-notebooks/vae/vae-ft-mse-840000-ema-pruned.ckpt\n",
      "additional VAE loaded\n",
      "Replace CrossAttention.forward to use NAI style Hypernetwork and xformers\n",
      "loading tokenizer\n",
      "prepare tokenizer\n",
      "pipeline is ready.\n",
      "iteration 1/1\n",
      "prompt 1/1: RAW photo, mirox in a fancy suit, fashion magazine photoshoot, full body shot, high detailed skin, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\n",
      "negative prompt: (weird eyes, disfigured eyes, looking different direction:1.3), cgi, 3d, render, mutated hands, mutated fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, bad quality, worst quality\n",
      "100%|███████████████████████████████████████████| 35/35 [00:08<00:00,  4.37it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.82it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.80it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.77it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.74it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.73it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.70it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.70it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.68it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.67it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.65it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:07<00:00,  4.64it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# @title ## 6.2. Inference\n",
    "v2 = False  # @param {type:\"boolean\"}\n",
    "v_parameterization = False  # @param {type:\"boolean\"}\n",
    "prompt = \"RAW photo, mirox in a fancy suit, fashion magazine photoshoot, full body shot, high detailed skin, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"  # @param {type: \"string\"}\n",
    "negative = \"(weird eyes, disfigured eyes, looking different direction:1.3), cgi, 3d, render, mutated hands, mutated fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, bad quality, worst quality\"  # @param {type: \"string\"}\n",
    "model = os.path.join(dreambooth_output_dir,'Miroslav7.ckpt')  # @param {type: \"string\"}\n",
    "vae = os.path.join(vae_dir,'vae-ft-mse-840000-ema-pruned.ckpt')  # @param {type: \"string\"}\n",
    "outdir = inference_dir  # @param {type: \"string\"}\n",
    "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
    "sampler = \"euler_a\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "steps = 35  # @param {type: \"slider\", min: 1, max: 100}\n",
    "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
    "width = 512  # @param {type: \"integer\"}\n",
    "height = 768  # @param {type: \"integer\"}\n",
    "images_per_prompt = 12  # @param {type: \"integer\"}\n",
    "batch_size = 1  # @param {type: \"integer\"}\n",
    "clip_skip = 1  # @param {type: \"slider\", min: 1, max: 40}\n",
    "seed = -1  # @param {type: \"integer\"}\n",
    "\n",
    "final_prompt = f\"{prompt} --n {negative}\"\n",
    "\n",
    "config = {\n",
    "    \"v2\": v2,\n",
    "    \"v_parameterization\": v_parameterization,\n",
    "    \"ckpt\": model,\n",
    "    \"outdir\": outdir,\n",
    "    \"xformers\": True,\n",
    "    \"vae\": vae if vae else None,\n",
    "    \"fp16\": True,\n",
    "    \"W\": width,\n",
    "    \"H\": height,\n",
    "    \"seed\": seed if seed > 0 else None,\n",
    "    \"scale\": scale,\n",
    "    \"sampler\": sampler,\n",
    "    \"steps\": steps,\n",
    "    \"max_embeddings_multiples\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"images_per_prompt\": images_per_prompt,\n",
    "    \"clip_skip\": clip_skip if not v2 else None,\n",
    "    \"prompt\": final_prompt,\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    if isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    if isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    if isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python gen_img_diffusers.py {args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec053eed-bc75-4fce-a6b4-1362f5e86a90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-01 15:52:46--  https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4374 (4.3K) [text/plain]\n",
      "Saving to: 'prune.py'\n",
      "\n",
      "prune.py            100%[===================>]   4.27K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-06-01 15:52:46 (43.5 MB/s) - 'prune.py' saved [4374/4374]\n",
      "\n",
      "Loading model from /home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/Hen1.ckpt\n",
      "Applying option fp16\n",
      "Saving pruned model to /home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/Hen1-fp16-pruned.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#@title ## 7.2. Model Pruner\n",
    "\n",
    "os.chdir(tools_dir)\n",
    "\n",
    "if not os.path.exists('prune.py'):\n",
    "    !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
    "\n",
    "#@markdown Convert to Float16\n",
    "fp16 = True #@param {'type':'boolean'}\n",
    "#@markdown Use EMA for weights\n",
    "ema = False #@param {'type':'boolean'}\n",
    "#@markdown Strip CLIP weights\n",
    "no_clip = False #@param {'type':'boolean'}\n",
    "#@markdown Strip VAE weights\n",
    "no_vae = False #@param {'type':'boolean'}\n",
    "#@markdown Strip depth model weights\n",
    "no_depth = False #@param {'type':'boolean'}\n",
    "#@markdown Strip UNet weights\n",
    "no_unet = False #@param {'type':'boolean'}\n",
    "\n",
    "model_path = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/Hen1.ckpt\" #@param {'type' : 'string'}\n",
    "\n",
    "config = {\n",
    "    \"fp16\": fp16,\n",
    "    \"ema\": ema,\n",
    "    \"no_clip\": no_clip,\n",
    "    \"no_vae\": no_vae,\n",
    "    \"no_depth\": no_depth,\n",
    "    \"no_unet\": no_unet,\n",
    "}\n",
    "\n",
    "suffixes = {\n",
    "    \"fp16\": \"-fp16\",\n",
    "    \"ema\": \"-ema\",\n",
    "    \"no_clip\": \"-no-clip\",\n",
    "    \"no_vae\": \"-no-vae\",\n",
    "    \"no_depth\": \"-no-depth\",\n",
    "    \"no_unet\": \"-no-unet\",\n",
    "}\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "\n",
    "dir_name = os.path.dirname(model_path)\n",
    "base_name = os.path.basename(model_path)\n",
    "output_name = base_name.split('.')[0]\n",
    "\n",
    "for option, suffix in suffixes.items():\n",
    "    if config[option]:\n",
    "        print(f\"Applying option {option}\")\n",
    "        output_name += suffix\n",
    "        \n",
    "output_name += '-pruned'\n",
    "output_path = os.path.join(dir_name, output_name + ('.ckpt' if model_path.endswith(\".ckpt\") else \".safetensors\"))\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python3 prune.py {model_path} {output_path} {args}\"\n",
    "!{final_args}\n",
    "\n",
    "print(f\"Saving pruned model to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39252798-c427-4770-a350-465cb6b62bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507a812acbc141c7ae3eb501f840c0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Login to Huggingface Hub</h3>'), Text(value='', description='Write Token:', sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title ## 7.1. Upload Config widgimport ipywidgets as widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create the widget elements\n",
    "write_token_input = widgets.Text(description=\"Write Token:\",style={\"description_width\": \"initial\"})\n",
    "orgs_name_input = widgets.Text(description=\"Orgs Name:\",style={\"description_width\": \"initial\"})\n",
    "model_name_input = widgets.Text(description=\"Model Name:\",style={\"description_width\": \"initial\"})\n",
    "dataset_name_input = widgets.Text(description=\"Dataset Name:\",style={\"description_width\": \"initial\"})\n",
    "make_private_checkbox = widgets.Checkbox(value=True, description=\"Make Private\")\n",
    "\n",
    "# Create the widget container\n",
    "widget_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Login to Huggingface Hub</h3>\"),\n",
    "    write_token_input,\n",
    "    orgs_name_input,\n",
    "    model_name_input,\n",
    "    dataset_name_input,\n",
    "    make_private_checkbox\n",
    "])\n",
    "\n",
    "display(widget_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8a51b3-8edb-441e-9e51-fcfee60905a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/studio-lab-user/.cache/huggingface/token\n",
      "Login successful\n",
      "Model repo 'xxthekingxx/Miroslav7' didn't exist, creating repo\n",
      "Model repo 'xxthekingxx/Miroslav7' link: https://huggingface.co/xxthekingxx/Miroslav7\n",
      "\n",
      "uploading to:  xxthekingxx/Miroslav7\n",
      "uploading config\n",
      "uploading sample images\n",
      "uploading sample image grid\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433ef82ec62d4426bd75e52a1768f155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Miroslav7_grid.png:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11b23708c1d4166a60a88004a2818db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# @title ## 7.1. Create repo\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
    "\n",
    "\n",
    "# @markdown Login to Huggingface Hub\n",
    "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
    "write_token = write_token_input.value\n",
    "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
    "orgs_name = orgs_name_input.value\n",
    "# @markdown If your model/dataset repo does not exist, it will automatically create it.\n",
    "model_name = model_name_input.value\n",
    "dataset_name = dataset_name_input.value\n",
    "make_private = make_private_checkbox.value  # @param{type:\"boolean\"}\n",
    "\n",
    "def authenticate(write_token):\n",
    "    login(write_token, add_to_git_credential=True)\n",
    "    api = HfApi()\n",
    "    return api.whoami(write_token), api\n",
    "\n",
    "\n",
    "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
    "    global model_repo\n",
    "    global datasets_repo\n",
    "    \n",
    "    if orgs_name == \"\":\n",
    "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
    "    else:\n",
    "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
    "\n",
    "    try:\n",
    "        validate_repo_id(repo_id)\n",
    "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
    "    except HfHubHTTPError as e:\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
    "    \n",
    "    if repo_type == \"model\":\n",
    "        model_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
    "    else:\n",
    "        datasets_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
    "\n",
    "user, api = authenticate(write_token)\n",
    "\n",
    "# @markdown This will be uploaded to model repo\n",
    "#model_path = os.path.join(dreambooth_output_dir,\"Hen1.ckpt\")  # @param {type :\"string\"}\n",
    "path_in_repo = \"\"  # @param {type :\"string\"}\n",
    "# @markdown Now you can save your config file for future use\n",
    "# @markdown Other Information\n",
    "commit_message = \"uploading model\"  # @param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "    commit_message = \"feat: upload \" + project_name.value + \" checkpoint\"\n",
    "\n",
    "if model_name:\n",
    "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
    "if dataset_name:\n",
    "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)\n",
    "\n",
    "print(\"uploading to: \",user[\"name\"] + \"/\" + model_name.strip())\n",
    "print(\"uploading config\")\n",
    "api.upload_folder(\n",
    "    folder_path=dreambooth_config_dir,\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"config\",\n",
    ")\n",
    "print(\"uploading sample images\")\n",
    "api.upload_folder(\n",
    "    folder_path=dreambooth_output_dir + \"/\" + \"sample\" + \"/\" + \"Miroslav7\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"samples\",\n",
    ")\n",
    "print(\"uploading sample image grid\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=dreambooth_output_dir + \"/\" + \"sample\" + \"/\" + \"Miroslav7_grid.png\",\n",
    "    path_in_repo=\"Miroslav7_grid.png\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    ")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38929646-2133-4704-a8c9-dbc6b1b45f7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166/4188737608.py\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_166/4188737608.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mupload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_166/4188737608.py\u001b[0m in \u001b[0;36mupload_model\u001b[0;34m(model_paths, is_folder, is_config)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{path_in_repo}_config\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{project_name.value}_config\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_folder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_name' is not defined"
     ]
    }
   ],
   "source": [
    "# @title ### 8.2.1. Upload Model\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# @markdown This will be uploaded to model repo\n",
    "#model_path = os.path.join(dreambooth_output_dir,\"Hen1.ckpt\")  # @param {type :\"string\"}\n",
    "path_in_repo = \"\"  # @param {type :\"string\"}\n",
    "# @markdown Now you can save your config file for future use\n",
    "config_path = dreambooth_config_dir  # @param {type :\"string\"}\n",
    "# @markdown Other Information\n",
    "commit_message = \"uploading model\"  # @param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "    commit_message = \"feat: upload \" + project_name.value + \" checkpoint\"\n",
    "\n",
    "#if os.path.exists(model_path):\n",
    "#    vae_exists = os.path.exists(os.path.join(model_path, \"vae\"))\n",
    "#    unet_exists = os.path.exists(os.path.join(model_path, \"unet\"))\n",
    "#    text_encoder_exists = os.path.exists(os.path.join(model_path, \"text_encoder\"))\n",
    "\n",
    "\n",
    "def upload_model(model_paths, is_folder: bool, is_config: bool):\n",
    "    path_obj = Path(model_paths)\n",
    "    trained_model = path_obj.parts[-1]\n",
    "\n",
    "    if path_in_repo:\n",
    "        trained_model = path_in_repo\n",
    "\n",
    "    if is_config:\n",
    "        if path_in_repo:\n",
    "            trained_model = f\"{path_in_repo}_config\"\n",
    "        else:\n",
    "            trained_model = f\"{project_name.value}_config\"\n",
    "\n",
    "    if is_folder == True:\n",
    "        print(f\"Uploading {trained_model} to https://huggingface.co/\" + model_repo)\n",
    "        print(f\"Please wait...\")\n",
    "\n",
    "        if vae_exists and unet_exists and text_encoder_exists:\n",
    "            api.upload_folder(\n",
    "                folder_path=model_paths,\n",
    "                repo_id=model_repo,\n",
    "                commit_message=commit_message,\n",
    "                ignore_patterns=\".ipynb_checkpoints\",\n",
    "            )\n",
    "        else:\n",
    "            api.upload_folder(\n",
    "                folder_path=model_paths,\n",
    "                path_in_repo=trained_model,\n",
    "                repo_id=model_repo,\n",
    "                commit_message=commit_message,\n",
    "                ignore_patterns=\".ipynb_checkpoints\",\n",
    "            )\n",
    "        print(\n",
    "            f\"Upload success, located at https://huggingface.co/\"\n",
    "            + model_repo\n",
    "            + \"/tree/main\\n\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Uploading {trained_model} to https://huggingface.co/\" + model_repo)\n",
    "        print(f\"Please wait...\")\n",
    "\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=model_paths,\n",
    "            path_in_repo=trained_model,\n",
    "            repo_id=model_repo,\n",
    "            commit_message=commit_message,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Upload success, located at https://huggingface.co/\"\n",
    "            + model_repo\n",
    "            + \"/blob/main/\"\n",
    "            + trained_model\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "def upload():\n",
    "    #if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
    "    #    upload_model(model_path, False, False)\n",
    "    #else:\n",
    "    #    upload_model(model_path, True, False)\n",
    "\n",
    "    if config_path:\n",
    "        upload_model(config_path, True, True)\n",
    "\n",
    "\n",
    "upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082353d0-cb86-4d05-8d23-0e3bb8d33ff5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
