{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f63422-b926-4761-bbd9-3422e91f33e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 DEFINE DIRECTORIES\n",
      "2.0 CLONE REPO AND INSTALL DIRECTORIES\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/accelerate_config/config.yaml\n",
      "kohya-trainer folder already exists\n",
      "Installation can take multiple minutes, enable \"Verbose\" to see progress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 23.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Files removed: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 14:23:13.188921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/studio-lab-user/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "Model already exists.\n",
      "Model already exists.\n",
      "4.1 DATA CLEANING + BLIP Captioning + Custom Caption/Tag\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/.ipynb_checkpoints\n",
      "None\n",
      "folder config not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6955.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been converted\n",
      "python /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/finetune/make_captions.py \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/.ipynb_checkpoints\" --batch_size=8 --beam_search --min_length=5 --max_length=75 --debug --caption_extension=\".caption\" --max_data_loader_n_workers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-13 14:23:27.194419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is:  /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer\n",
      "load images from /home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/.ipynb_checkpoints\n",
      "found 0 images.\n",
      "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "BLIP loaded\n",
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/prepare_config.ini\n",
      "folder config exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 230.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been converted\n",
      "python /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/finetune/make_captions.py \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1\" --batch_size=8 --beam_search --min_length=5 --max_length=75 --debug --caption_extension=\".caption\" --max_data_loader_n_workers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-13 14:23:59.033763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is:  /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer\n",
      "load images from /home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1\n",
      "found 11 images.\n",
      "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "BLIP loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/1.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/10.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/11.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/2.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/3.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/4.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/5.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/6.png a man with a beard and a tie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/7.png a man with a beard and a tie\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/8.png a man with a beard and a tie making a funny face\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro1/9.png a man with a beard and a tie\n",
      "done!\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2\n",
      "None\n",
      "folder config not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 537.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been converted\n",
      "python /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer/finetune/make_captions.py \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2\" --batch_size=8 --beam_search --min_length=5 --max_length=75 --debug --caption_extension=\".caption\" --max_data_loader_n_workers=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-06-13 14:24:35.019943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory is:  /home/studio-lab-user/sagemaker-studiolab-notebooks/kohya-trainer\n",
      "load images from /home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2\n",
      "found 14 images.\n",
      "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "BLIP loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/12.png a man with a beard and a tie\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/13.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/14.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/15.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/16.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/17.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/18.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/19.png a man with a beard and a white shirt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/20.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/21.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/22.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/23.png a man with a beard and a white shirt\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/24.png a man with a beard and a tie\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miro2/25.png a man with a beard and a tie\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import configparser\n",
    "import random\n",
    "import concurrent.futures\n",
    "import time\n",
    "from library.studiolabs_utils import (\n",
    "    clone_or_update_repo,\n",
    "    install_dependencies,\n",
    "    create_dirs,\n",
    "    download_model,\n",
    "    preProcessingParams,\n",
    "    clean_directory,\n",
    "    process_image,\n",
    "    find_images,\n",
    "    convertImages,\n",
    "    preprocess_images,\n",
    "    run_captioning_process,\n",
    "    custom_caption_tag\n",
    ")\n",
    "\n",
    "\n",
    "print('1.0 DEFINE DIRECTORIES')\n",
    "dirs = create_dirs()\n",
    "print('2.0 CLONE REPO AND INSTALL DIRECTORIES')\n",
    "# Read the config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(dirs['trainer_config'])\n",
    "print(dirs['accelerate_config'])\n",
    "    \n",
    "clone_or_update_repo(\n",
    "    url=config.get('UserSettings', 'repo_url'),\n",
    "    save_directory=dirs['root_dir'],\n",
    "    branch = config.get('UserSettings', 'branch')\n",
    "    )\n",
    "\n",
    "\n",
    "install_dependencies(\n",
    "    dirs,\n",
    "    verbose=config.getboolean('UserSettings', 'verbose'), \n",
    "    install_xformers=config.getboolean('UserSettings', 'install_xformers')\n",
    "    )\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "command = \"pip cache purge\"\n",
    "subprocess.run(command, shell=True)\n",
    "command = \"accelerate config default\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "\n",
    "# Get the parameter values from the config file\n",
    "model_url = config.get('DownloadModels', 'model_url')\n",
    "vae_url = config.get('DownloadModels', 'vae_url')\n",
    "\n",
    "\n",
    "# Download the model file if it doesn't exist\n",
    "\n",
    "try:\n",
    "    download_model(model_url, dirs['pretrained_dir'])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download the model from {model_url}. Error: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    download_model(vae_url, dirs['vae_dir'])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download the VAE model from {vae_url}. Error: {str(e)}\")\n",
    "\n",
    "\n",
    "print('4.1 DATA CLEANING + BLIP Captioning + Custom Caption/Tag')\n",
    "# Use BLIP for general images\n",
    "# Use Waifu for anime/manga images\n",
    "# Specified in the config file\n",
    "subfolders = [os.path.join(dirs['train_data_dir'], f) for f in os.listdir(dirs['train_data_dir']) if os.path.isdir(os.path.join(dirs['train_data_dir'], f))]\n",
    "\n",
    "for folder in subfolders:\n",
    "    print(folder)\n",
    "    ini_files = [filename for filename in os.listdir(folder) if filename.endswith('.ini')]\n",
    "    prepare_config_file = os.path.join(folder, 'prepare_config.ini')\n",
    "    if prepare_config_file in ini_files:\n",
    "        subfolder_config_path = prepare_config_file\n",
    "    elif ini_files:\n",
    "        subfolder_config_path = os.path.join(folder, ini_files[0])\n",
    "    else:\n",
    "        subfolder_config_path = None\n",
    "\n",
    "    print(subfolder_config_path)\n",
    "    \n",
    "    param_config = configparser.ConfigParser()\n",
    "    if subfolder_config_path is not None:\n",
    "        print('folder config exists')\n",
    "        param_config.read(subfolder_config_path)\n",
    "    else:\n",
    "        print('folder config not exists')\n",
    "        param_config.read(dirs['trainer_config'])\n",
    "        \n",
    "    convert = param_config.get('ImagePreprocessing', 'convert')\n",
    "    random_color = param_config.get('ImagePreprocessing', 'random_color')\n",
    "    recursive = param_config.get('ImagePreprocessing', 'recursive')\n",
    "\n",
    "    batch_size, supported_types, background_colors = preProcessingParams()\n",
    "    clean_directory(folder, supported_types)\n",
    "    images = find_images(folder)\n",
    "    num_batches = len(images) // batch_size + 1\n",
    "    convertImages(images,convert,batch_size,num_batches)\n",
    "    run_captioning_process(param_config, folder, dirs['finetune_dir'])\n",
    "    custom_caption_tag(param_config, folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b9f6d-99d1-407b-9ba7-1d35c7d0abb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ## 5.5. Start Training\n",
    "\n",
    "#@markdown Check your config here if you want to edit something: \n",
    "#@markdown - `sample_prompt` : /content/dreambooth/config/sample_prompt.txt\n",
    "#@markdown - `config_file` : /content/dreambooth/config/config_file.toml\n",
    "#@markdown - `dataset_config` : /content/dreambooth/config/dataset_config.toml\n",
    "\n",
    "#@markdown Generated sample can be seen here: /content/dreambooth/output/sample\n",
    "\n",
    "#@markdown You can import config from another session if you want.\n",
    "\n",
    "import toml\n",
    "print(dirs)\n",
    "\n",
    "sample_prompt = os.path.join(dirs['repo_dir'],\"sample_prompt.txt\") #@param {type:'string'}\n",
    "config_file = os.path.join(dirs['repo_dir'],\"config_file.toml\") #@param {type:'string'}\n",
    "dataset_config = os.path.join(dirs['repo_dir'],\"dataset_config.toml\") #@param {type:'string'}\n",
    "\n",
    "with open(config_file, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "    \n",
    "config['model_arguments']['pretrained_model_name_or_path'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/pretrained_model/Realistic_Vision_V2.0-fp16-no-ema.safetensors\"\n",
    "config['model_arguments']['vae'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/vae/vae-ft-mse-840000-ema-pruned.ckpt\"\n",
    "config['huggingface_arguments']['huggingface_path_in_repo'] = \"mymodel\"\n",
    "config['huggingface_arguments']['huggingface_token'] = \"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\"\n",
    "config['huggingface_arguments']['huggingface_repo_id'] = \"xxthekingxx/myMiroslav8\"\n",
    "config['training_arguments']['output_name'] = \"Miroslav8\"\n",
    "config['training_arguments']['log_prefix'] = \"Miroslav8\"\n",
    "config['training_arguments']['max_train_steps'] = 400\n",
    "config['optimizer_arguments']['learning_rate'] = 4e-6\n",
    "    \n",
    "with open(config_file, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(dataset_config, 'r') as configfile:\n",
    "    config = toml.load(configfile)\n",
    "\n",
    "print(config['datasets'][0]['subsets'][0]['image_dir'])\n",
    "config['datasets'][0]['subsets'][0]['image_dir'] = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/train_data/Miroslav3\"\n",
    "\n",
    "with open(dataset_config, 'w') as configfile:\n",
    "    toml.dump(config, configfile)\n",
    "\n",
    "\n",
    "accelerate_conf = {\n",
    "    \"config_file\" : dirs['accelerate_config'],\n",
    "    \"num_cpu_threads_per_process\" : 1,\n",
    "}\n",
    "\n",
    "train_conf = {\n",
    "    \"sample_prompts\" : sample_prompt,\n",
    "    \"dataset_config\" : dataset_config,\n",
    "    \"config_file\" : config_file\n",
    "}\n",
    "\n",
    "def train(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    return args\n",
    "\n",
    "accelerate_args = train(accelerate_conf)\n",
    "train_args = train(train_conf)\n",
    "final_args = f\"accelerate launch {accelerate_args} train_db.py {train_args}\"\n",
    "print(final_args)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b044b58-21c1-4f6b-9c5a-0edc677a1071",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def get_font():\n",
    "    # Get the list of font names\n",
    "    font_names = [f.name for f in fm.fontManager.ttflist]\n",
    "    #print([(i,k) for i,k in enumerate(font_names)])\n",
    "    font_path = fm.findfont(font_names[17])\n",
    "    fontsize = 40\n",
    "    font = ImageFont.truetype(font_path, fontsize)\n",
    "    return font, fontsize\n",
    "\n",
    "def fetch_image_locations(directory):\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = [f for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "    return image_locations\n",
    "\n",
    "def main():\n",
    "    top_space = 40\n",
    "    left_space = 400\n",
    "    text_offset = 4\n",
    "    padding = 20  # Padding between images\n",
    "    text_width_limit = 750 \n",
    "    # Directory path and grid settings\n",
    "    directory = '/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav8/'\n",
    "    grid_save_dir = \"/home/studio-lab-user/sagemaker-studiolab-notebooks/dreambooth/output/sample/Miroslav8_grid.png\"\n",
    "\n",
    "\n",
    "    # Fetch image files from the directory\n",
    "    image_locations = fetch_image_locations(directory)\n",
    "    prompts = sorted(list(set([img_name.split('_')[1] for img_name in image_locations])))\n",
    "    epochs = sorted(list(set([img_name.split('_')[2][1:] for img_name in image_locations])))\n",
    "    font, fontsize = get_font()\n",
    "    \n",
    "    max_image_width = 0\n",
    "    max_image_height = 0\n",
    "\n",
    "    # Find the maximum width and height among all images\n",
    "    for image_file in image_locations:\n",
    "        image_path = os.path.join(directory, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        max_image_width = max(max_image_width, width)\n",
    "        max_image_height = max(max_image_height, height)\n",
    "\n",
    "    canvas_width = ((max_image_width + padding) * len(epochs)) + padding + left_space\n",
    "    canvas_height = ((max_image_height + padding) * len(prompts)) + padding + top_space\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Iterate over the image files and place them in the grid\n",
    "    for i,epoch in enumerate(epochs):\n",
    "        epoch_x = ((max_image_width + padding) * i) + padding + left_space \n",
    "        epoch_y = text_offset \n",
    "        draw.text((epoch_x, epoch_y), f'Epoch: {epoch}', fill='black', font=font)\n",
    "        for k,prompt in enumerate(prompts):\n",
    "        \n",
    "            text=prompt\n",
    "            #Shorten the text if it exceeds 50 characters\n",
    "            if len(text) > 200:\n",
    "                text = text[:200] + '...'\n",
    "\n",
    "            # Wrap the text to fit within the limit\n",
    "            wrapped_text = textwrap.wrap(text, width=int(text_width_limit / fontsize))\n",
    "\n",
    "            # Calculate the total height required for the wrapped text\n",
    "            total_text_height = len(wrapped_text) * fontsize\n",
    "\n",
    "            # Calculate the starting position to center the text vertically\n",
    "            y_start = top_space + padding + k * (padding + max_image_height)\n",
    "            x = text_offset * 4\n",
    "\n",
    "            # Draw the wrapped text\n",
    "            for line in wrapped_text:\n",
    "                text_bbox = draw.textbbox((0, y_start), line, font=font)\n",
    "                text_width = text_bbox[2] - text_bbox[0]\n",
    "                text_height = text_bbox[3] - text_bbox[1]\n",
    "                draw.text((x, y_start), line, font=font, fill='black')\n",
    "                y_start += fontsize\n",
    "        \n",
    "            for img_name in image_locations:\n",
    "                if epoch == img_name.split('_')[2][1:] and prompt == img_name.split('_')[1]:\n",
    "                    image_path = os.path.join(directory, img_name)\n",
    "                    img = Image.open(image_path)\n",
    "\n",
    "                    # Calculate the position of the image in the grid\n",
    "                    x = (max_image_width + padding) * i + padding + left_space\n",
    "                    y = top_space + (max_image_height + padding) * k + padding\n",
    "\n",
    "                    # Paste the image onto the canvas\n",
    "                    canvas.paste(img, (x, y))\n",
    "\n",
    "    # Save the final image grid\n",
    "    canvas.save(grid_save_dir)\n",
    "    print('Saved')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39252798-c427-4770-a350-465cb6b62bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 7.1. Upload Config widgimport ipywidgets as widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create the widget elements\n",
    "write_token_input = widgets.Text(description=\"Write Token:\",style={\"description_width\": \"initial\"})\n",
    "orgs_name_input = widgets.Text(description=\"Orgs Name:\",style={\"description_width\": \"initial\"})\n",
    "model_name_input = widgets.Text(description=\"Model Name:\",style={\"description_width\": \"initial\"})\n",
    "dataset_name_input = widgets.Text(description=\"Dataset Name:\",style={\"description_width\": \"initial\"})\n",
    "make_private_checkbox = widgets.Checkbox(value=True, description=\"Make Private\")\n",
    "\n",
    "# Create the widget container\n",
    "widget_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Login to Huggingface Hub</h3>\"),\n",
    "    write_token_input,\n",
    "    orgs_name_input,\n",
    "    model_name_input,\n",
    "    dataset_name_input,\n",
    "    make_private_checkbox\n",
    "])\n",
    "\n",
    "display(widget_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a51b3-8edb-441e-9e51-fcfee60905a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 7.1. Create repo\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
    "\n",
    "\n",
    "# @markdown Login to Huggingface Hub\n",
    "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
    "write_token = write_token_input.value\n",
    "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
    "orgs_name = orgs_name_input.value\n",
    "# @markdown If your model/dataset repo does not exist, it will automatically create it.\n",
    "model_name = model_name_input.value\n",
    "dataset_name = dataset_name_input.value\n",
    "make_private = make_private_checkbox.value  # @param{type:\"boolean\"}\n",
    "\n",
    "def authenticate(write_token):\n",
    "    login(write_token, add_to_git_credential=True)\n",
    "    api = HfApi()\n",
    "    return api.whoami(write_token), api\n",
    "\n",
    "\n",
    "def create_repo(api, user, orgs_name, repo_name, repo_type, make_private=False):\n",
    "    global model_repo\n",
    "    global datasets_repo\n",
    "    \n",
    "    if orgs_name == \"\":\n",
    "        repo_id = user[\"name\"] + \"/\" + repo_name.strip()\n",
    "    else:\n",
    "        repo_id = orgs_name + \"/\" + repo_name.strip()\n",
    "\n",
    "    try:\n",
    "        validate_repo_id(repo_id)\n",
    "        api.create_repo(repo_id=repo_id, repo_type=repo_type, private=make_private)\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' didn't exist, creating repo\")\n",
    "    except HfHubHTTPError as e:\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' exists, skipping create repo\")\n",
    "    \n",
    "    if repo_type == \"model\":\n",
    "        model_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
    "    else:\n",
    "        datasets_repo = repo_id\n",
    "        print(f\"{repo_type.capitalize()} repo '{repo_id}' link: https://huggingface.co/datasets/{repo_id}\\n\")\n",
    "\n",
    "user, api = authenticate(write_token)\n",
    "\n",
    "# @markdown This will be uploaded to model repo\n",
    "#model_path = os.path.join(dreambooth_output_dir,\"Hen1.ckpt\")  # @param {type :\"string\"}\n",
    "path_in_repo = \"\"  # @param {type :\"string\"}\n",
    "# @markdown Now you can save your config file for future use\n",
    "# @markdown Other Information\n",
    "commit_message = \"uploading model\"  # @param {type :\"string\"}\n",
    "\n",
    "if not commit_message:\n",
    "    commit_message = \"feat: upload \" + project_name.value + \" checkpoint\"\n",
    "\n",
    "if model_name:\n",
    "    create_repo(api, user, orgs_name, model_name, \"model\", make_private)\n",
    "if dataset_name:\n",
    "    create_repo(api, user, orgs_name, dataset_name, \"dataset\", make_private)\n",
    "\n",
    "print(\"uploading to: \",user[\"name\"] + \"/\" + model_name.strip())\n",
    "print(\"uploading config\")\n",
    "api.upload_folder(\n",
    "    folder_path=dirs['dreambooth_config_dir'],\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"config\",\n",
    ")\n",
    "print(\"uploading sample images\")\n",
    "api.upload_folder(\n",
    "    folder_path=dirs['dreambooth_output_dir'] + \"/\" + \"sample\" + \"/\" + \"Miroslav8\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    "    path_in_repo=\"samples\",\n",
    ")\n",
    "print(\"uploading sample image grid\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=dirs['dreambooth_output_dir'] + \"/\" + \"sample\" + \"/\" + \"Miroslav8_grid.png\",\n",
    "    path_in_repo=\"Miroslav8_grid.png\",\n",
    "    repo_id=user[\"name\"] + \"/\" + model_name.strip(),\n",
    "    repo_type=None,\n",
    ")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937f88d-819a-4e35-bc36-353198d18048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## 6.2. Inference\n",
    "v2 = False  # @param {type:\"boolean\"}\n",
    "v_parameterization = False  # @param {type:\"boolean\"}\n",
    "prompt = \"RAW photo, mirox in a fancy suit, fashion magazine photoshoot, full body shot, high detailed skin, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"  # @param {type: \"string\"}\n",
    "negative = \"(weird eyes, disfigured eyes, looking different direction:1.3), cgi, 3d, render, mutated hands, mutated fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, bad quality, worst quality\"  # @param {type: \"string\"}\n",
    "model = os.path.join(dreambooth_output_dir,'Miroslav7.ckpt')  # @param {type: \"string\"}\n",
    "vae = os.path.join(vae_dir,'vae-ft-mse-840000-ema-pruned.ckpt')  # @param {type: \"string\"}\n",
    "outdir = inference_dir  # @param {type: \"string\"}\n",
    "scale = 7  # @param {type: \"slider\", min: 1, max: 40}\n",
    "sampler = \"euler_a\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "steps = 35  # @param {type: \"slider\", min: 1, max: 100}\n",
    "precision = \"fp16\"  # @param [\"fp16\", \"bf16\"] {allow-input: false}\n",
    "width = 512  # @param {type: \"integer\"}\n",
    "height = 768  # @param {type: \"integer\"}\n",
    "images_per_prompt = 12  # @param {type: \"integer\"}\n",
    "batch_size = 1  # @param {type: \"integer\"}\n",
    "clip_skip = 1  # @param {type: \"slider\", min: 1, max: 40}\n",
    "seed = -1  # @param {type: \"integer\"}\n",
    "\n",
    "final_prompt = f\"{prompt} --n {negative}\"\n",
    "\n",
    "config = {\n",
    "    \"v2\": v2,\n",
    "    \"v_parameterization\": v_parameterization,\n",
    "    \"ckpt\": model,\n",
    "    \"outdir\": outdir,\n",
    "    \"xformers\": True,\n",
    "    \"vae\": vae if vae else None,\n",
    "    \"fp16\": True,\n",
    "    \"W\": width,\n",
    "    \"H\": height,\n",
    "    \"seed\": seed if seed > 0 else None,\n",
    "    \"scale\": scale,\n",
    "    \"sampler\": sampler,\n",
    "    \"steps\": steps,\n",
    "    \"max_embeddings_multiples\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"images_per_prompt\": images_per_prompt,\n",
    "    \"clip_skip\": clip_skip if not v2 else None,\n",
    "    \"prompt\": final_prompt,\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    if isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    if isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    if isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python gen_img_diffusers.py {args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
